{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torchvision\n",
    "from torchvision.transforms import ToTensor, Compose, Normalize\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from dataset import ImageDataset, DSubset, Label, get_channel_means_stdevs\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_map = {1: 'Airplane', 2: 'Automobile', 3: 'Bird', 4: 'Cat', 5: 'Deer', 6: 'Dog', 7: 'Frog', 8: 'Horse', 9: 'Ship', 10: 'Truck'}\n",
    "\n",
    "with open('../results/channel_training_statistics.pkl', 'rb') as f:\n",
    "    training_channel_means, training_channel_stdevs = pickle.load(f)\n",
    "    \n",
    "tf = Compose([\n",
    "    Normalize(training_channel_means, training_channel_stdevs)\n",
    "])\n",
    "\n",
    "label_type = Label.REAL_OR_SYNTHETIC\n",
    "\n",
    "train_dataset = ImageDataset(DSubset.TRAIN, label_type, transform = tf)\n",
    "test_dataset = ImageDataset(DSubset.TEST, label_type, transform = tf)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size = 64, shuffle = True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size = 64, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_channels = 3\n",
    "# train_channel_means, train_channel_stdevs = get_channel_means_stdevs(train_dataloader, num_channels = num_channels, verbose = False)\n",
    "\n",
    "# # Verify successful standardization functionality: mean 0 and standard deviation 1 on training set\n",
    "# assert np.allclose(np.array(train_channel_means), np.zeros(num_channels), atol = 1e-5)\n",
    "# assert np.allclose(np.array(train_channel_stdevs), np.ones(num_channels), atol = 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SyntheticCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Convolutional Neural Network to classify images of being either real \n",
    "    or synthetically (AI) generated.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        SyntheticCNN initializer.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(3, 5, 5), # [BATCH_SIZE, 3, 32, 32] -> [BATCH_SIZE, 5, 28, 28]\n",
    "            nn.BatchNorm2d(5),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.MaxPool2d(2, 2), # [BATCH_SIZE, 5, 28, 28] -> [BATCH_SIZE, 5, 14, 14]\n",
    "            nn.Conv2d(5, 15, 5), # [BATCH_SIZE, 5, 14, 14] -> [BATCH_SIZE, 15, 10, 10]\n",
    "            nn.BatchNorm2d(15),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.MaxPool2d(2, 2), # [BATCH_SIZE, 15, 10, 10] -> [BATCH_SIZE, 15, 5, 5]\n",
    "        )\n",
    "\n",
    "        self.linear_layers = nn.Sequential(\n",
    "            nn.Linear(15 * 5 * 5, 64),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(16, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        SyntheticCNN forward method. Runs convolutional layers first, then\n",
    "        converts to linear layers.\n",
    "\n",
    "        Args:\n",
    "            x -- The input to be passed through the network.\n",
    "\n",
    "        Returns:\n",
    "            x -- The output of the model.\n",
    "        \"\"\"\n",
    "        \n",
    "        x = self.conv_layers(x)\n",
    "        x = torch.flatten(x, start_dim = 1) # Flatten all dimensions except batch (dim 0)\n",
    "        x = self.linear_layers(x)\n",
    "\n",
    "        return x # logit of class 1 (synthetic) likelihood\n",
    "    \n",
    "synthetic_model = SyntheticCNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_cross_entropy = nn.BCEWithLogitsLoss()\n",
    "\n",
    "sgd = torch.optim.SGD(synthetic_model.parameters(), lr = 1e-3, momentum = 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_train_loop(dataloader: DataLoader, model: nn.Module, loss_fn: nn.Module, optimizer: torch.optim.Optimizer):\n",
    "    \"\"\"\n",
    "    Runs one full epoch of training on model.\n",
    "\n",
    "    Args:\n",
    "        dataloader -- The DataLoader through which to produce instances.\n",
    "        model -- The model to be used for label prediction on instances.\n",
    "        loss_fn -- The loss function, for backpropagation\n",
    "        optimizer -- The optimizer, for reducing loss\n",
    "\n",
    "    Returns:\n",
    "        average_epoch_loss -- The model loss this epoch, averaged by the number of instances in the dataset\n",
    "        epoch_accuracy -- The model accuracy this epoch, averaged by the number of instances in the dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    model.train()\n",
    "\n",
    "    num_correct_total = 0\n",
    "    epoch_loss = 0.0\n",
    "\n",
    "    for i, (X, y) in enumerate(dataloader):\n",
    "\n",
    "        pred = model(X)\n",
    "        batch_loss = loss_fn(pred.squeeze(), y.float())\n",
    "\n",
    "        batch_loss.backward()\n",
    "        \n",
    "        epoch_loss += batch_loss.item()\n",
    "\n",
    "        num_correct_in_batch = torch.sum((torch.sigmoid(pred.detach().squeeze()) > 0.5).float() == y.float()).item()\n",
    "        num_correct_total += num_correct_in_batch\n",
    "\n",
    "        # print(f'Batch {i+1} | Loss: {batch_loss.item():>7f} | Accuracy: {num_correct_in_batch / len(y):>7}')\n",
    "\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    average_epoch_loss = epoch_loss / len(dataloader.dataset)\n",
    "    epoch_accuracy = num_correct_total / len(dataloader.dataset)\n",
    "\n",
    "    return average_epoch_loss, epoch_accuracy\n",
    "\n",
    "\n",
    "def run_test_loop(dataloader: DataLoader, model: nn.Module, loss_fn: nn.Module):\n",
    "    \"\"\"\n",
    "    Runs one full dataset-worth of testing on model.\n",
    "\n",
    "    Args:\n",
    "        dataloader -- The DataLoader through which to produce instances.\n",
    "        model -- The model to be used for label prediction on instances.\n",
    "        loss_fn -- The loss function, for improvement checking\n",
    "\n",
    "    Returns:\n",
    "        average_epoch_loss -- The model loss this epoch, averaged by the number of instances in the dataset\n",
    "        epoch_accuracy -- The model accuracy this epoch, averaged by the number of instances in the dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    num_correct = 0\n",
    "    epoch_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for X, y in dataloader:\n",
    "\n",
    "            pred = model(X)\n",
    "            batch_loss = loss_fn(pred.squeeze(), y.float())\n",
    "\n",
    "            epoch_loss += batch_loss.item()\n",
    "            num_correct += torch.sum((torch.sigmoid(pred.detach().squeeze()) > 0.5).float() == y.float()).item()\n",
    "\n",
    "    average_epoch_loss = epoch_loss / len(dataloader.dataset)\n",
    "    epoch_accuracy = num_correct / len(dataloader.dataset)\n",
    "\n",
    "    return average_epoch_loss, epoch_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Train Loss: 0.008483 | Train Accuracy: 0.702440\n",
      "Epoch 1 | Test Loss: 0.006170 | Test Accuracy: 0.823850\n",
      "---------------------------------------------------------\n",
      "Epoch 2 | Train Loss: 0.004899 | Train Accuracy: 0.867770\n",
      "Epoch 2 | Test Loss: 0.004879 | Test Accuracy: 0.866500\n",
      "---------------------------------------------------------\n",
      "Epoch 3 | Train Loss: 0.004121 | Train Accuracy: 0.891700\n",
      "Epoch 3 | Test Loss: 0.003963 | Test Accuracy: 0.895900\n",
      "---------------------------------------------------------\n",
      "Epoch 4 | Train Loss: 0.003741 | Train Accuracy: 0.902800\n",
      "Epoch 4 | Test Loss: 0.003759 | Test Accuracy: 0.900300\n",
      "---------------------------------------------------------\n",
      "Epoch 5 | Train Loss: 0.003520 | Train Accuracy: 0.908880\n",
      "Epoch 5 | Test Loss: 0.003273 | Test Accuracy: 0.918300\n",
      "---------------------------------------------------------\n",
      "Epoch 6 | Train Loss: 0.003359 | Train Accuracy: 0.913900\n",
      "Epoch 6 | Test Loss: 0.003196 | Test Accuracy: 0.917950\n",
      "---------------------------------------------------------\n",
      "Epoch 7 | Train Loss: 0.003236 | Train Accuracy: 0.917740\n",
      "Epoch 7 | Test Loss: 0.003454 | Test Accuracy: 0.907750\n",
      "---------------------------------------------------------\n",
      "Epoch 8 | Train Loss: 0.003157 | Train Accuracy: 0.919610\n",
      "Epoch 8 | Test Loss: 0.003269 | Test Accuracy: 0.915900\n",
      "---------------------------------------------------------\n",
      "Epoch 9 | Train Loss: 0.003051 | Train Accuracy: 0.921840\n",
      "Epoch 9 | Test Loss: 0.002968 | Test Accuracy: 0.924850\n",
      "---------------------------------------------------------\n",
      "Epoch 10 | Train Loss: 0.002991 | Train Accuracy: 0.923690\n",
      "Epoch 10 | Test Loss: 0.003098 | Test Accuracy: 0.918900\n",
      "----------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "\n",
    "for i in range(EPOCHS):\n",
    "    train_loss, train_accuracy, = run_train_loop(train_dataloader, synthetic_model, binary_cross_entropy, sgd)\n",
    "    test_loss, test_accuracy = run_test_loop(test_dataloader, synthetic_model, binary_cross_entropy)\n",
    "\n",
    "    print(f'Epoch {i+1:>3} | Train Loss: {train_loss:>10f} | Train Accuracy: {train_accuracy:>10f}')\n",
    "    print(f'Epoch {i+1:>3} | Test Loss: {test_loss:>10f} | Test Accuracy: {test_accuracy:>10f}')\n",
    "    print('-' * len(f'Epoch {i+1} | Train Loss: {train_loss:>10f} | Train Accuracy: {train_accuracy:>10f}'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
